
# Proyecto: Servidor FastAPI Dockerizado para Clasificaci贸n y Respuesta de Preguntas Basadas en LLM

Este repositorio contiene el c贸digo fuente para un servidor API que utiliza FastAPI y cliente de LLM con la clave API de Groq para clasificar preguntas en categor铆as espec铆ficas y generar respuestas basadas en prompts.

## Estructura del Repositorio

```bash
 /app
  main.py               # Archivo principal que ejecuta la API usando FastAPI
  agents.py             # Archivo que contiene la l贸gica de integraci贸n con LLM
  requirements.txt      # Archivo que contiene las dependencias necesarias para ejecutar el proyecto
  tests.py              # Archivo que contiene que incluye pruebas unitarias
  Dockerfile            # Archivo para contenerizar la aplicaci贸n con Docker
  docker-compose.yml    # Archivo para levantar el servicio usando Docker Compose
```

## Descripci贸n de los Archivos

- **main.py**: Define los endpoints de la API y gestiona las solicitudes de clasificaci贸n y generaci贸n de respuestas.
- **agents.py**: Contiene la l贸gica para la clasificaci贸n de preguntas y generaci贸n de respuestas a trav茅s del LLM.
- **requirements.txt**: Lista las dependencias del proyecto, incluyendo FastAPI, LangChain, entre otras.
- **Dockerfile**: Define c贸mo crear la imagen Docker para el proyecto.
- **docker-compose.yml**: Facilita el despliegue del entorno utilizando Docker Compose.

## Requisitos

- Docker instalado en su m谩quina.
- Python 3.12 o superior.
- Clave de API de Groq.
- Para las pruebas unitarias: Pytest 8.3.2

## Configuraci贸n

1. **Clonar el repositorio**:
   ```bash
   git clone https://github.com/usuario/FastAPI-LLM-Server.git
   cd FastAPI-LLM-Server
   ```

2. **Instalar las dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configurar el archivo `.env`**:
   Crear un archivo `.env` en el directorio principal del proyecto con el siguiente contenido, reemplazando `your-groq-api-key` con tu clave de API de Groq:
   ```env
   GROQ_API_KEY='your-groq-api-key'
   ```

4. **Construir y ejecutar con Docker**:
   ```bash
   docker-compose up --build
   ```

5. **Acceder a la API**:
   La API estar谩 disponible en `http://localhost:8000`, para ver la documentaci贸n interactiva o enviar peticiones POST al endpoint `/preguntar/`.

   La respuesta ser谩 un JSON con la clasificaci贸n de la pregunta y la respuesta generada por el LLM.

## Pruebas Unitarias

El proyecto incluye pruebas unitarias para verificar el correcto funcionamiento del servicio, asegurando la clasificaci贸n de preguntas y la generaci贸n de respuestas en tests.py. Las preguntas utilizadas en las pruebas son preguntas ejemplares que abarcan las categor铆as legal, contable, m茅dica y preguntas no clasificables.

Para ejecutar las pruebas:

```bash
pytest tests.py
```

Las pruebas verifican:

- La correcta clasificaci贸n de las preguntas (legal, contable, m茅dica, y otras preguntas no clasificables).
- La generaci贸n de respuestas basadas en los prompts proporcionados.
- El manejo adecuado de preguntas no clasificadas, asegurando que el sistema devuelva un c贸digo de estado 400 y el mensaje de error apropiado: "No se pudo clasificar la pregunta.".

Tambi茅n tienes la opci贸n de ejecutar el Test como un script de Python, para ver los resultados espec铆ficos de las preguntas ejemplares generados por el LLM:

```bash
python tests.py
```
